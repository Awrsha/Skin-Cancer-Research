{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["ut6ZVSP6Gb4F","peZaBR-_nhCl"],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8564242,"sourceType":"datasetVersion","datasetId":5119916}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Requirements","metadata":{"id":"by1o5v5UDBtj"}},{"cell_type":"code","source":"import cv2\nimport shutil\nimport random\nimport zipfile\nimport warnings\nfrom PIL import Image\nimport numpy as np\n# %load_ext cudf.pandas\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.applications.resnet import ResNet50\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tensorflow.keras.callbacks import CSVLogger\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"id":"TWChDwKVCjJ7","outputId":"c778b8b1-4ec2-4e72-e0ad-d304d40f225e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balance the dataset by removing excess samples from the majority class\nfrom random import sample\nmin_path = \"/kaggle/input/skin-canser-b584m584/Melanoma-b584m584/malignant\"\nmaj_path = \"/kaggle/input/skin-canser-b584m584/Melanoma-b584m584/benign\"\naddress = [image for image in os.listdir(maj_path)]\ncut = len(os.listdir(min_path))\ncut_list = sample(address, cut)\nfor index in tqdm(os.listdir(maj_path)):\n    if index not in cut_list:\n        os.remove(os.path.join(maj_path, index))\n\n# Verify the number of samples\nbenigns = len(os.listdir(\"/kaggle/input/skin-canser-b584m584/Melanoma-b584m584/benign\"))\nmelignant = len(os.listdir(\"/kaggle/input/skin-canser-b584m584/Melanoma-b584m584/malignant\"))\nprint(f\"\\nNumber of benign Samples: {benigns}\\nNumber of malignant Samples: {melignant}\")","metadata":{"id":"WERv5Wrohmmf","outputId":"645d09b5-1a66-4f09-d701-db05ebb0270f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here since the path to dataset is read-only we need to copy them to another Dir\nnew_path = '/kaggle/working/root'\nos.makedirs(new_path, exist_ok=True)\nshutil.copytree('/kaggle/input/skin-canser-b584m584/Melanoma-b584m584', new_path, dirs_exist_ok=True)\n# Just to Verify\nprint(os.listdir(new_path))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to divide a test set\ndef divide_test_set(temp_path, cut_percentage):\n    \"\"\"\n    Returns a list containing relative address for images you need to move\n    ------------------------------------------------------------------------\n    temp_path: \n                    a path to the root directory of your data\n    \n    cut_precentage: \n                    how much data you want to move\"\"\"\n    \n    rel_image_paths = [os.path.join(temp_path, i) for i in os.listdir(temp_path)]\n    cut_set = random.sample(rel_image_paths, int(cut_percentage * len(os.listdir(temp_path))))\n    return cut_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def copy_data(input_list, path):\n    \"\"\"Copies all the data located at the input list indexes\n    -------------------------------------------------------------\n    input_list: \n                a list containing all the relative paths\n    \n    path:\n                output directory\"\"\"\n    \n    os.makedirs(path, exist_ok=True)\n    for index in (input_list):\n        shutil.copy(index, path) ","metadata":{"id":"FA7qjcoimOXj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def move_data(input_list, path):\n    \"\"\"moves all the data located at the input list indexes\n    -------------------------------------------------------------\n    input_list: \n                a list containing all the relative paths\n    \n    path:\n                output directory\"\"\"\n    \n    os.makedirs(path, exist_ok=True)\n    for index in (input_list):\n        shutil.move(index, path) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_remainder(input_list, root_path):\n    \"\"\"\n    This function removes the samples presented in input_list from the root path files\n    -------------------------------------------------------------------------------------\n    input_list: files to be excluded\n    root_path: directory path\n    \n    \"\"\"\n    all_paths = [os.path.join(root_path, i) for i in os.listdir(root_path)]\n    # creates a new list that only includes items from all_paths that are not in input_list\n    filtered_list = [image for image in all_paths if image not in input_list]\n    \n    return filtered_list\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data generator for training, validation and testing\ndef create_generator(DIR):\n    datagen = ImageDataGenerator(rescale=1/255)\n    generator = datagen.flow_from_directory(directory=DIR,\n                                            batch_size=batch_size,\n                                            class_mode='binary',\n                                            target_size=(224, 224))\n    return generator","metadata":{"id":"Xteh2dlNMGjK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_me(root):\n    \"\"\"\n    This function captures prediction/label pairs and return predictions, labels lists\n    ------------------------------------------------\n    root: \n            a relative path to the test directory\"\"\"\n    predictions = []\n    labels = []\n    for label in tqdm(os.listdir(root)):\n        if label == \"malignant\":\n            new_root = os.path.join(root, label)\n            for image in tqdm(os.listdir(new_root)):\n                # read, covert, and normalize the image\n                img_path = os.path.join(new_root, image)\n                image_file = Image.open(img_path).convert('RGB')\n                image_array = np.array(image_file)\n                # image_array = image_array * 255.0/image_array.max()\n                image_array = cv2.resize(image_array, (224,224))\n                image_array = image_array / 255.0\n                image_array = image_array.reshape(1, 224,224, 3)\n                # Prediction\n                predictions.append(model.predict(image_array, verbose=0).squeeze())\n                labels.append(1)\n                \n            return predictions, labels \n\n\n        elif label == \"benign\":\n            new_root = os.path.join(root, label)\n            for image in tqdm(os.listdir(new_root)):\n                # read, covert, and normalize the image\n                img_path = os.path.join(new_root, image)\n                image_file = Image.open(img_path).convert('RGB')\n                image_array = np.array(image_file)\n                # image_array = image_array * 255.0/image_array.max()\n                image_array = cv2.resize(image_array, (224,224))\n                image_array = image_array / 255.0\n                image_array = image_array.reshape(1, 224,224, 3)\n                # Prediction\n                predictions.append(model.predict(image_array, verbose=0).squeeze())\n                labels.append(0)\n            \n            return predictions, labels \n\n        else:\n            return \"\\nSomething is not right!\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to build the model\ndef build_model():\n    image_size = 224\n    ResNet50_base = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3))\n    model = ResNet50_base.output\n    model = tf.keras.layers.GlobalAveragePooling2D()(model)\n    model = tf.keras.layers.Dropout(rate=0.4)(model)\n    model = tf.keras.layers.Dense(1, activation='sigmoid')(model)\n    model = tf.keras.models.Model(inputs=ResNet50_base.input, outputs=model)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training parameters\nbatch_size = 4\nEPOCHS = 40\ntest_accs = []\ntest_recalls = []\ntest_precisions = []\ntps = []\nfps = []\ntns = []\nfns = []\n\n\n# Ensure TensorFlow uses the GPU\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\n\nfor i in tqdm(range(6)):\n    # TEST SAMPLING -----------------------------------------------\n    # Pick a sample of images for testing \n    input_path = \"/kaggle/working/root/benign\"\n    benign_test_set = divide_test_set(input_path, 0.20)\n    input_path = \"/kaggle/working/root/malignant\"\n    malignant_test_set = divide_test_set(input_path, 0.20)\n    \n    # Copy selected testing images into their corresponding folder\n    copy_data(input_list = benign_test_set , path = f\"/kaggle/working/test_{i+1}/benign\")\n    copy_data(input_list = malignant_test_set , path = f\"/kaggle/working/test_{i+1}/malignant\")\n    \n    # TRAIN SAMPLING ----------------------------------------------\n    # Constant Paths\n    root_benign_dir = \"/kaggle/working/root/benign\"\n    root_malignant_dir = \"/kaggle/working/root/malignant\"\n    \n    # Get the training samples indexes\n    benign_train_set = set_remainder(benign_test_set, root_benign_dir)\n    malignant_train_set = set_remainder(malignant_test_set, root_malignant_dir)\n    \n    # Copy selected training images into their corresponding folder\n    copy_data(input_list = benign_train_set , path = f\"/kaggle/working/train_{i+1}/benign\")\n    copy_data(input_list = malignant_train_set , path = f\"/kaggle/working/train_{i+1}/malignant\")\n    \n    # VALIDATION SAMPLING -----------------------------------------\n    # Pick a sample of images for validation \n    input_path = f\"/kaggle/working/train_{i+1}/benign\"\n    benign_val_set = divide_test_set(input_path, 0.20)\n    input_path = f\"/kaggle/working/train_{i+1}/malignant\"\n    malignant_val_set = divide_test_set(input_path, 0.20)\n    \n    # Move selected validation images into their corresponding folder\n    move_data(input_list = benign_val_set , path = f\"/kaggle/working/val_{i+1}/benign\")\n    move_data(input_list = malignant_val_set , path = f\"/kaggle/working/val_{i+1}/malignant\")\n    print(f\"\\n{i+1} out of 6 Dataset splitted.\")\n    \n    # Used for Fit() function \n    va = len(os.listdir(f\"/kaggle/working/val_{i+1}/benign\")) # benign valiiation\n    tr = len(os.listdir(f\"/kaggle/working/train_{i+1}/benign\")) # benign train\n    va_ = len(os.listdir(f\"/kaggle/working/val_{i+1}/malignant\")) # malignant validation\n    tr_ = len(os.listdir(f\"/kaggle/working/train_{i+1}/malignant\")) # malignant train\n    print(f\"\\nNumber of benign train Samples: {tr}\\nNumber of benign validation Samples: {va}\")\n    print(f\"\\nNumber of malignant train Samples: {tr_}\\nNumber of malignant validation Samples: {va_}\\n\")\n    \n    # Create data generators\n    Train_Dir = f\"/kaggle/working/train_{i+1}/\"\n    train_generator = create_generator(DIR=Train_Dir)\n    Val_Dir = f\"/kaggle/working/val_{i+1}/\"\n    validation_generator = create_generator(DIR=Val_Dir)\n    \n    # Build a model and pick an optimizer\n    model = build_model()\n    opt = Adam(learning_rate=0.001)\n    \n    # Callback and Logger\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=4, min_delta=0.0001, mode='auto', verbose=1)\n    csv_logger = CSVLogger(f'/kaggle/working/model_version_{i+1}_log.csv', append=True, separator=',')\n    \n    # Compile and Run\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', 'Recall', 'Precision'])\n    history = model.fit(\n        train_generator,\n        steps_per_epoch= (tr+tr_)//batch_size,\n        epochs=EPOCHS,\n        verbose=1,\n        validation_data=validation_generator,\n        callbacks=[reduce_lr, csv_logger]\n    )\n    \n    model.save_weights(f'/kaggle/working/model_version_{i+1}.weights.h5')\n    \n    # Testing begins here ...\n    test_path = f\"/kaggle/working/test_{i+1}/\"\n    test_generator = create_generator(DIR=test_path)\n    \n    # Capture common metrics\n    c_metrics = model.evaluate(test_generator)\n    test_accs.append(c_metrics[1])\n    test_recalls.append(c_metrics[2])\n    test_precisions.append(c_metrics[3])\n    \n    # Make predictions\n    predictions, labels = test_me(test_path)\n    new_list = [0 if value <= 0.50 else 1 for value in predictions]\n    \n    cf = confusion_matrix(labels, new_list)\n    tn = cf[0, 0]  # True Negatives\n    fp = cf[0, 1]  # False Positives\n    fn = cf[1, 0]  # False Negatives\n    tp = cf[1, 1]  # True Positives\n\n    # Just logging everything\n    print(f\"\\nTraining number {i+1}/6 model performance:\\nTN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\\n\")\n    print(f\"Test ACC: {c_metrics[1]}, Test RECALL: {c_metrics[2]}, Test PRECISION: {c_metrics[3]}\\n\")\n    tns.append(tn)\n    fps.append(fp)\n    fns.append(fn)\n    tps.append(tp)\n    print(\"=\"*100)\n    \n    # Clear the path for the next training index\n    shutil.rmtree(f\"/kaggle/working/train_{i+1}/\")\n    shutil.rmtree(f\"/kaggle/working/val_{i+1}/\")\n    shutil.rmtree(f\"/kaggle/working/test_{i+1}/\")\n    del model\n    del history\n    del train_generator, validation_generator, test_generator\n    tf.keras.backend.clear_session()\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}